{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e6f779d1-2c24-42f5-89cf-b3655d38e837",
      "metadata": {
        "id": "e6f779d1-2c24-42f5-89cf-b3655d38e837"
      },
      "source": [
        "## Module 8: Homework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d9a68d99-b412-4f1e-a0c9-48736ffa5904",
      "metadata": {
        "id": "d9a68d99-b412-4f1e-a0c9-48736ffa5904"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a788632a-c0af-42e1-912b-4ebe7431e05a",
      "metadata": {
        "id": "a788632a-c0af-42e1-912b-4ebe7431e05a"
      },
      "source": [
        "#### Random Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dfd6055f-209a-4c85-a6a2-6b41a37b5260",
      "metadata": {
        "id": "dfd6055f-209a-4c85-a6a2-6b41a37b5260"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f86fb1b0-6862-4b87-92ff-5f928fe145ee",
      "metadata": {
        "id": "f86fb1b0-6862-4b87-92ff-5f928fe145ee"
      },
      "source": [
        "#### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8eb1d159-002a-4ccd-a45c-da28df030f47",
      "metadata": {
        "id": "8eb1d159-002a-4ccd-a45c-da28df030f47"
      },
      "outputs": [],
      "source": [
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "be46aad1-4d12-4f8a-8f75-d6df2659d586",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be46aad1-4d12-4f8a-8f75-d6df2659d586",
        "outputId": "52cc78ed-d04f-4461-adb1-502aea7fb979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:\n",
            "Dataset ImageFolder\n",
            "    Number of datapoints: 800\n",
            "    Root location: data/train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=(200, 200), interpolation=bilinear, max_size=None, antialias=True)\n",
            "               ToTensor()\n",
            "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            "           )\n",
            "Val:\n",
            "Dataset ImageFolder\n",
            "    Number of datapoints: 201\n",
            "    Root location: data/test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=(200, 200), interpolation=bilinear, max_size=None, antialias=True)\n",
            "               ToTensor()\n",
            "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            "           )\n"
          ]
        }
      ],
      "source": [
        "train_data = datasets.ImageFolder(\n",
        "    root=\"data/train\",\n",
        "    transform=data_transform,\n",
        "    target_transform=None\n",
        ")\n",
        "\n",
        "val_data = datasets.ImageFolder(\n",
        "    root=\"data/test\",\n",
        "    transform=data_transform\n",
        ")\n",
        "\n",
        "print(f\"Train:\\n{train_data}\\nVal:\\n{val_data}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "91a46142-6772-444a-bac4-5a98784cc753",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91a46142-6772-444a-bac4-5a98784cc753",
        "outputId": "fbde371c-d253-43cb-eb58-dab9ba72495d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['curly', 'straight']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_data.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7a235e8f-97d5-4c44-9378-ceff860484ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a235e8f-97d5-4c44-9378-ceff860484ec",
        "outputId": "3816c71c-f44a-4996-ff26-105aec0429ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 200, 200])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "img, label = train_data[0][0], train_data[0][1]\n",
        "img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "85fb898e-8998-45ea-923f-63fba28d4e32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85fb898e-8998-45ea-923f-63fba28d4e32",
        "outputId": "26747d53-c7f7-4180-c58f-a2fd5e9d5f48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7ab14d66c740>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7ab14e185760>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train_dl = DataLoader(\n",
        "    dataset=train_data,\n",
        "    batch_size=20,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_dl = DataLoader(\n",
        "    dataset=val_data,\n",
        "    batch_size=20,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "train_dl, val_dl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17689cfd-93dc-4d13-a818-efdbbbbd70c6",
      "metadata": {
        "id": "17689cfd-93dc-4d13-a818-efdbbbbd70c6"
      },
      "source": [
        "#### Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d2d59275-118e-4516-bc31-fe0f67a4a5cc",
      "metadata": {
        "id": "d2d59275-118e-4516-bc31-fe0f67a4a5cc"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32*99*99, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block(x)\n",
        "        output = self.classifier(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "993c9406-94a3-42d3-8782-609ce19cc120",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "993c9406-94a3-42d3-8782-609ce19cc120",
        "outputId": "79d6ef9f-0d28-46f0-cec4-ac380b24e6b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetwork(\n",
              "  (conv_block): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=313632, out_features=64, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model = NeuralNetwork()\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a36d674-4259-4e89-b7c9-f9a3a424d8c5",
      "metadata": {
        "id": "8a36d674-4259-4e89-b7c9-f9a3a424d8c5"
      },
      "source": [
        "> Q1: Which loss function you will use?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1c1b0ccd-8d51-4dad-a5e8-504f10bf52d6",
      "metadata": {
        "id": "1c1b0ccd-8d51-4dad-a5e8-504f10bf52d6"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)\n",
        "criterion = nn.BCEWithLogitsLoss() # for binary classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c7fe9bf-9965-4527-bf33-f9e367586cc4",
      "metadata": {
        "id": "2c7fe9bf-9965-4527-bf33-f9e367586cc4"
      },
      "source": [
        "> Q2: The total number of parameters using?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a1ecce0b-3231-43b5-8776-c09bf1504f15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1ecce0b-3231-43b5-8776-c09bf1504f15",
        "outputId": "0465d3f3-7baa-4c5f-eb2f-6fd532f35721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 20073473\n"
          ]
        }
      ],
      "source": [
        "# Option 2: Manual counting\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fa14bed-44ac-4e1b-843e-c9a3bf03e759",
      "metadata": {
        "id": "8fa14bed-44ac-4e1b-843e-c9a3bf03e759"
      },
      "source": [
        "#### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c38bb88d-20ee-4b7d-8f9c-ed5e2d897f4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c38bb88d-20ee-4b7d-8f9c-ed5e2d897f4f",
        "outputId": "f1d3b00d-f2fc-466e-9a54-fb4a21d91c35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c62db0a3-9fbc-45d9-9c04-3c6381ad6ab9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c62db0a3-9fbc-45d9-9c04-3c6381ad6ab9",
        "outputId": "c86fe65a-7a03-43d4-b3b8-47eebfc3c700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.6705, Acc: 0.6000, Val Loss: 0.6094, Val Acc: 0.6468\n",
            "Epoch 2/10, Loss: 0.5593, Acc: 0.7037, Val Loss: 0.5892, Val Acc: 0.6915\n",
            "Epoch 3/10, Loss: 0.5207, Acc: 0.7312, Val Loss: 0.5878, Val Acc: 0.6517\n",
            "Epoch 4/10, Loss: 0.4628, Acc: 0.7688, Val Loss: 0.6816, Val Acc: 0.6368\n",
            "Epoch 5/10, Loss: 0.4259, Acc: 0.8013, Val Loss: 0.6590, Val Acc: 0.6716\n",
            "Epoch 6/10, Loss: 0.3505, Acc: 0.8438, Val Loss: 0.8655, Val Acc: 0.6468\n",
            "Epoch 7/10, Loss: 0.3027, Acc: 0.8725, Val Loss: 0.6974, Val Acc: 0.6816\n",
            "Epoch 8/10, Loss: 0.2523, Acc: 0.8900, Val Loss: 0.7179, Val Acc: 0.7114\n",
            "Epoch 9/10, Loss: 0.2289, Acc: 0.9163, Val Loss: 0.7017, Val Acc: 0.6866\n",
            "Epoch 10/10, Loss: 0.2004, Acc: 0.9263, Val Loss: 1.0504, Val Acc: 0.6915\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "history1 = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for images, labels in train_dl:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_data)\n",
        "    epoch_acc = correct_train / total_train\n",
        "    history1['loss'].append(epoch_loss)\n",
        "    history1['acc'].append(epoch_acc)\n",
        "\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_dl:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            labels = labels.float().unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(val_data)\n",
        "    val_epoch_acc = correct_val / total_val\n",
        "    history1['val_loss'].append(val_epoch_loss)\n",
        "    history1['val_acc'].append(val_epoch_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
        "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aaf5ff5-d820-4ef2-92df-2e01e7bc0ebf",
      "metadata": {
        "id": "4aaf5ff5-d820-4ef2-92df-2e01e7bc0ebf"
      },
      "source": [
        "> The median of training accuracy for all the epochs for this model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8139ea35-2244-4952-8371-8b3fb3eb90d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8139ea35-2244-4952-8371-8b3fb3eb90d8",
        "outputId": "c6e131a0-4522-40b5-f599-670595e8c36d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.8225)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "np.median(history1['acc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e80d0be7-8ed1-48ae-b5a9-99ec9b51e774",
      "metadata": {
        "id": "e80d0be7-8ed1-48ae-b5a9-99ec9b51e774"
      },
      "source": [
        "> The standard deviation of training loss for all the epochs for this model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0c2a21bc-6561-4f71-9d55-3157dbae8b77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c2a21bc-6561-4f71-9d55-3157dbae8b77",
        "outputId": "45307c22-e9a9-4a24-e268-10b80ff9b857"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.14849673501797045)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "np.std(history1['loss'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9acba5ca-eeb8-4d9a-b86d-bd243fa957b8",
      "metadata": {
        "id": "9acba5ca-eeb8-4d9a-b86d-bd243fa957b8"
      },
      "source": [
        "#### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2f7674c2-e483-46a0-9fa6-0c042d69b65c",
      "metadata": {
        "id": "2f7674c2-e483-46a0-9fa6-0c042d69b65c"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.RandomRotation(50),\n",
        "    transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ee8f0ff5-27f9-4f8a-9291-578db1dc9a87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee8f0ff5-27f9-4f8a-9291-578db1dc9a87",
        "outputId": "405127ba-cf9e-4456-9620-91dc43848efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:\n",
            "Dataset ImageFolder\n",
            "    Number of datapoints: 800\n",
            "    Root location: ./data/train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=(200, 200), interpolation=bilinear, max_size=None, antialias=True)\n",
            "               RandomRotation(degrees=[-50.0, 50.0], interpolation=nearest, expand=False, fill=0)\n",
            "               RandomResizedCrop(size=(200, 200), scale=(0.9, 1.0), ratio=(0.9, 1.1), interpolation=bilinear, antialias=True)\n",
            "               RandomHorizontalFlip(p=0.5)\n",
            "               ToTensor()\n",
            "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            "           )\n",
            "Val:\n",
            "Dataset ImageFolder\n",
            "    Number of datapoints: 201\n",
            "    Root location: ./data/test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=(200, 200), interpolation=bilinear, max_size=None, antialias=True)\n",
            "               ToTensor()\n",
            "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            "           )\n"
          ]
        }
      ],
      "source": [
        "train_data = datasets.ImageFolder(\n",
        "    root=\"./data/train\",\n",
        "    transform=train_transform,\n",
        "    target_transform=None\n",
        ")\n",
        "\n",
        "val_data = datasets.ImageFolder(\n",
        "    root=\"./data/test\",\n",
        "    transform=val_transform\n",
        ")\n",
        "\n",
        "print(f\"Train:\\n{train_data}\\nVal:\\n{val_data}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "97bd8164-6fc7-446f-85c6-b6b89ab58e02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97bd8164-6fc7-446f-85c6-b6b89ab58e02",
        "outputId": "82138101-1c8f-4a2c-c5ca-b99080d497e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7ab14d6fd370>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7ab14d707860>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "train_dl = DataLoader(\n",
        "    dataset=train_data,\n",
        "    batch_size=20,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_dl = DataLoader(\n",
        "    dataset=val_data,\n",
        "    batch_size=20,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "train_dl, val_dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "41e1e532-8f80-431b-9f72-a4101c26d95c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41e1e532-8f80-431b-9f72-a4101c26d95c",
        "outputId": "98364f4f-b225-4639-a086-fca7dce1d3ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.6302, Acc: 0.6713, Val Loss: 0.6168, Val Acc: 0.6766\n",
            "Epoch 2/10, Loss: 0.6037, Acc: 0.6575, Val Loss: 0.6981, Val Acc: 0.7015\n",
            "Epoch 3/10, Loss: 0.5916, Acc: 0.6575, Val Loss: 0.6259, Val Acc: 0.6915\n",
            "Epoch 4/10, Loss: 0.5697, Acc: 0.6787, Val Loss: 0.6705, Val Acc: 0.7015\n",
            "Epoch 5/10, Loss: 0.5486, Acc: 0.6987, Val Loss: 0.7482, Val Acc: 0.6816\n",
            "Epoch 6/10, Loss: 0.5508, Acc: 0.7125, Val Loss: 0.6131, Val Acc: 0.7264\n",
            "Epoch 7/10, Loss: 0.5347, Acc: 0.7188, Val Loss: 0.5627, Val Acc: 0.7512\n",
            "Epoch 8/10, Loss: 0.5105, Acc: 0.7300, Val Loss: 0.5742, Val Acc: 0.7214\n",
            "Epoch 9/10, Loss: 0.5190, Acc: 0.7400, Val Loss: 0.6401, Val Acc: 0.6716\n",
            "Epoch 10/10, Loss: 0.4902, Acc: 0.7562, Val Loss: 0.5761, Val Acc: 0.7214\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "history2 = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for images, labels in train_dl:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_data)\n",
        "    epoch_acc = correct_train / total_train\n",
        "    history2['loss'].append(epoch_loss)\n",
        "    history2['acc'].append(epoch_acc)\n",
        "\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_dl:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            labels = labels.float().unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(val_data)\n",
        "    val_epoch_acc = correct_val / total_val\n",
        "    history2['val_loss'].append(val_epoch_loss)\n",
        "    history2['val_acc'].append(val_epoch_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
        "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The mean of test loss for all the epochs for the model trained with augmentations?"
      ],
      "metadata": {
        "id": "IPYm6x3kZvzK"
      },
      "id": "IPYm6x3kZvzK"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "45439e3f-1489-4caf-a6f6-da9d9167794b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45439e3f-1489-4caf-a6f6-da9d9167794b",
        "outputId": "2adbac0a-ecb7-49be-822d-f891362493fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.6325862905872401)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "np.mean(history2['val_loss'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The average of test accuracy for the last 5 epochs (from 6 to 10) for the model trained with augmentations?"
      ],
      "metadata": {
        "id": "Q0hwhrPfZ5ON"
      },
      "id": "Q0hwhrPfZ5ON"
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(history2['val_acc'][5:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6feTjo9ZYC5r",
        "outputId": "da8bd9fd-6b03-48b6-da78-d3a5cb90c559"
      },
      "id": "6feTjo9ZYC5r",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.718407960199005)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kft1BnGIa5ZK"
      },
      "id": "Kft1BnGIa5ZK",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml-zoomcamp-playground",
      "language": "python",
      "name": "ml-zoomcamp-playground"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}