{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nkdZlAV0PjT4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/alexeygrigorev/clothing-dataset-small.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKwZsQFhPxEP",
        "outputId": "6a8ad276-0579-4a5b-dec6-a16e9675c388"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'clothing-dataset-small'...\n",
            "remote: Enumerating objects: 3839, done.\u001b[K\n",
            "remote: Counting objects: 100% (400/400), done.\u001b[K\n",
            "remote: Compressing objects: 100% (400/400), done.\u001b[K\n",
            "remote: Total 3839 (delta 9), reused 385 (delta 0), pack-reused 3439 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3839/3839), 100.58 MiB | 44.07 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 224\n",
        "\n",
        "train_gen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    shear_range=10,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "train_ds = train_gen.flow_from_directory(\n",
        "    \"./clothing-dataset-small/train\",\n",
        "    target_size=(input_size, input_size),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "val_gen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "val_ds = val_gen.flow_from_directory(\n",
        "    \"./clothing-dataset-small/validation\",\n",
        "    target_size=(input_size, input_size),\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTN_cIJGP7G3",
        "outputId": "75ca9124-6e43-4c74-9480-b0e6183d7b62"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3068 images belonging to 10 classes.\n",
            "Found 341 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(input_size=150, learning_rate=0.01, size_inner=100, droprate=0.5):\n",
        "  base_model = Xception(\n",
        "      weights='imagenet',\n",
        "      include_top=False,\n",
        "      input_shape=(input_size, input_size, 3)\n",
        "  )\n",
        "  base_model.trainable = False\n",
        "\n",
        "  input = keras.Input(shape=(input_size, input_size, 3))\n",
        "\n",
        "  # base with pretrained conv layers\n",
        "  base = base_model(input, training=False)\n",
        "  # vec rep\n",
        "  vector = keras.layers.GlobalAveragePooling2D()(base)\n",
        "\n",
        "  # relu layer\n",
        "  inner = keras.layers.Dense(size_inner, activation=\"relu\")(vector)\n",
        "  drop = keras.layers.Dropout(droprate)(inner)\n",
        "\n",
        "  # dense layer/ logit output\n",
        "  output = keras.layers.Dense(10)(drop)\n",
        "\n",
        "  model = keras.Model(input, output)\n",
        "\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "  loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=loss,\n",
        "      metrics=[\"accuracy\"]\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "7TEgpEEPRdq2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "    \"xception_v4_1_{epoch:02d}_{val_accuracy:.3f}.keras\",\n",
        "    save_best_only=True,\n",
        "    monitor=\"val_accuracy\",\n",
        "    mode=\"max\"\n",
        ")"
      ],
      "metadata": {
        "id": "YtJ4_Y-LWWlF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 224\n",
        "learning_rate = 0.0005\n",
        "size = 100\n",
        "droprate = 0.2\n",
        "\n",
        "model = make_model(\n",
        "    input_size=input_size,\n",
        "    learning_rate=learning_rate,\n",
        "    size_inner=size,\n",
        "    droprate=droprate\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=10,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[checkpoint]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSVbYWF9UafR",
        "outputId": "828d4f3c-c1e6-4e89-e98c-33dd2ca704de"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 774ms/step - accuracy: 0.5546 - loss: 1.3259 - val_accuracy: 0.7977 - val_loss: 0.6055\n",
            "Epoch 2/10\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 421ms/step - accuracy: 0.7687 - loss: 0.6526 - val_accuracy: 0.8182 - val_loss: 0.5150\n",
            "Epoch 3/10\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 414ms/step - accuracy: 0.8198 - loss: 0.5119 - val_accuracy: 0.8006 - val_loss: 0.5334\n",
            "Epoch 4/10\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 419ms/step - accuracy: 0.8480 - loss: 0.4583 - val_accuracy: 0.8270 - val_loss: 0.4804\n",
            "Epoch 5/10\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 423ms/step - accuracy: 0.8473 - loss: 0.4434 - val_accuracy: 0.8504 - val_loss: 0.4570\n",
            "Epoch 6/10\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 417ms/step - accuracy: 0.8735 - loss: 0.3786 - val_accuracy: 0.8358 - val_loss: 0.4662\n",
            "Epoch 7/10\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 418ms/step - accuracy: 0.8817 - loss: 0.3411 - val_accuracy: 0.8328 - val_loss: 0.4525\n",
            "Epoch 8/10\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 415ms/step - accuracy: 0.8850 - loss: 0.3306 - val_accuracy: 0.8416 - val_loss: 0.4452\n",
            "Epoch 9/10\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 432ms/step - accuracy: 0.8891 - loss: 0.3177 - val_accuracy: 0.8416 - val_loss: 0.4419\n",
            "Epoch 10/10\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 420ms/step - accuracy: 0.9003 - loss: 0.2902 - val_accuracy: 0.8534 - val_loss: 0.4396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qI0LeMJHXOrX"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}