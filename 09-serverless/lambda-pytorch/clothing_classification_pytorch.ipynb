{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5WFelX0Zgxcn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch import nn, optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms, models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/alexeygrigorev/clothing-dataset-small.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOVqD8M_kA0W",
        "outputId": "8a463f2d-d6ed-43ab-bfb9-4e7de31e7440"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'clothing-dataset-small'...\n",
            "remote: Enumerating objects: 3839, done.\u001b[K\n",
            "remote: Counting objects: 100% (400/400), done.\u001b[K\n",
            "remote: Compressing objects: 100% (400/400), done.\u001b[K\n",
            "remote: Total 3839 (delta 9), reused 385 (delta 0), pack-reused 3439 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3839/3839), 100.58 MiB | 14.84 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading"
      ],
      "metadata": {
        "id": "sankj1rblEo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 224\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((input_size, input_size)),\n",
        "\n",
        "    # image transformations\n",
        "    transforms.RandomRotation(10), # Equivalent to shear_range\n",
        "    transforms.RandomResizedCrop(input_size, scale=(0.9, 1.0)), # Equivalent to zoom_range\n",
        "    transforms.RandomHorizontalFlip(), # Equivalent to horizontal_flip\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean = [0.485, 0.456, 0.406],\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((input_size, input_size)),\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean = [0.485, 0.456, 0.406],\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "train_ds = ImageFolder(\n",
        "    root=\"./clothing-dataset-small/train\",\n",
        "    transform=train_transforms,\n",
        "    target_transform=None\n",
        ")\n",
        "\n",
        "val_ds = ImageFolder(\n",
        "    root=\"./clothing-dataset-small/validation\",\n",
        "    transform=val_transforms\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=32,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_ds,\n",
        "    batch_size=32,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\\nVal batches: {len(val_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xM1fIi5iEnG",
        "outputId": "688ded32-92f3-404e-8863-f8270a4cbf54"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 96\n",
            "Val batches: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ],
      "metadata": {
        "id": "kszSKaXclAi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClothingClassifier(nn.Module):\n",
        "  def __init__(self, size_inner=100, droprate=0.2, num_classes=10):\n",
        "    super().__init__()\n",
        "    self.base_model = models.mobilenet_v2(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "    self.base_model.classifier = nn.Identity() # remove the mobilenet classifier\n",
        "\n",
        "    self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    self.classification = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(1280, size_inner),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(droprate),\n",
        "        nn.Linear(size_inner, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.base_model.features(x) # access features\n",
        "    x = self.pool(x)\n",
        "    x = self.classification(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "JQbH6nWij81O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "size = 32\n",
        "droprate = 0.2\n",
        "\n",
        "model = ClothingClassifier(\n",
        "    size_inner=size,\n",
        "    droprate=droprate,\n",
        "    num_classes=len(train_ds.classes)\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "print(f\"model is on {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKmTFwR3krYQ",
        "outputId": "e1acdaa6-ba97-4955-c040-509234b34051"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 194MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model is on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.0001\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "RQ-OZ2lTsqAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(train_loader):\n",
        "    running_loss, total_predictions, correct_predictions = 0., 0, 0\n",
        "\n",
        "    for i, (inputs, labels) in tqdm(enumerate(train_loader), desc=\"Training\"):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        total_predictions += labels.size(0)\n",
        "        correct_predictions += outputs.argmax(1).eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    return epoch_loss, epoch_accuracy"
      ],
      "metadata": {
        "id": "lCWnZQ7At-Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_one_epoch(val_loader):\n",
        "  vrunning_loss, vtotal_predictions, vcorrect_predictions = 0., 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, (vinputs, vlabels) in enumerate(val_loader):\n",
        "      vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
        "      voutputs = model(vinputs)\n",
        "      vloss = criterion(voutputs, vlabels)\n",
        "\n",
        "      vrunning_loss += vloss.item()\n",
        "      vtotal_predictions += vlabels.size(0)\n",
        "      vcorrect_predictions += voutputs.argmax(1).eq(vlabels).sum().item()\n",
        "\n",
        "  vepoch_loss = vrunning_loss / len(train_loader)\n",
        "  vepoch_accuracy = vcorrect_predictions / vtotal_predictions\n",
        "\n",
        "  return vepoch_loss, vepoch_accuracy\n"
      ],
      "metadata": {
        "id": "bHd3OTkRXQPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model"
      ],
      "metadata": {
        "id": "PrSWDabxk1eR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "best_val_accuracy = 0.\n",
        "checkpoint_path = 'mobilenet_v2_v1_{epoch:02d}_{val_accuracy:.3f}.pth'\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(f\"Epoch: {epoch + 1}\")\n",
        "\n",
        "  # training\n",
        "  model.train()\n",
        "  train_result = train_one_epoch(train_loader=train_loader)\n",
        "  print(f\"Train Loss: {train_result[0]} Train Accuracy: {train_result[1]}\")\n",
        "\n",
        "  # validating\n",
        "  model.eval()\n",
        "  val_result = validate_one_epoch(val_loader=val_loader)\n",
        "  print(f\"Val Loss: {val_result[0]} Val Accuracy: {val_result[1]}\")\n",
        "\n",
        "  # checkpointing\n",
        "  if val_result[1] > best_val_accuracy:\n",
        "    best_val_accuracy = val_result[1]\n",
        "    checkpoint = checkpoint_path.format(epoch=epoch + 1,\n",
        "                                       val_accuracy=val_result[1])\n",
        "    torch.save(model.state_dict(), checkpoint)\n",
        "    print(f\"Checkpoint saved to {checkpoint}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bVHaa5yyfp5",
        "outputId": "25968560-9c4e-4757-874d-058638d07cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 96it [00:30,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5396366917217771 Train Accuracy: 0.840612777053455\n",
            "Val Loss: 0.03889287660907333 Val Accuracy: 0.8914956011730205\n",
            "Checkpoint saved to mobilenet_v2_v1_01_0.891.pth\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 96it [00:31,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.33076863960983854 Train Accuracy: 0.9002607561929595\n",
            "Val Loss: 0.03204190063600739 Val Accuracy: 0.8973607038123167\n",
            "Checkpoint saved to mobilenet_v2_v1_02_0.897.pth\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 96it [00:30,  3.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.24424195215882114 Train Accuracy: 0.93122555410691\n",
            "Val Loss: 0.029134521590700995 Val Accuracy: 0.9178885630498533\n",
            "Checkpoint saved to mobilenet_v2_v1_03_0.918.pth\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 96it [00:31,  3.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1814966912691792 Train Accuracy: 0.9524119947848761\n",
            "Val Loss: 0.032093258089541145 Val Accuracy: 0.9178885630498533\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 96it [00:31,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.13622834091074765 Train Accuracy: 0.9638200782268579\n",
            "Val Loss: 0.03427098707955641 Val Accuracy: 0.8944281524926686\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 96it [00:30,  3.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.11270405936132495 Train Accuracy: 0.969361147327249\n",
            "Val Loss: 0.025823801557028975 Val Accuracy: 0.9149560117302052\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 96it [00:31,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.09809441672405228 Train Accuracy: 0.9729465449804433\n",
            "Val Loss: 0.03013465239200741 Val Accuracy: 0.9120234604105572\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 96it [00:30,  3.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.08418927516322583 Train Accuracy: 0.9755541069100391\n",
            "Val Loss: 0.030503496126281487 Val Accuracy: 0.906158357771261\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 96it [00:30,  3.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.057053528541776664 Train Accuracy: 0.9850065189048239\n",
            "Val Loss: 0.028968871411052532 Val Accuracy: 0.9032258064516129\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 96it [00:30,  3.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0641610156162642 Train Accuracy: 0.984354628422425\n",
            "Val Loss: 0.02946635974512901 Val Accuracy: 0.9208211143695014\n",
            "Checkpoint saved to mobilenet_v2_v1_10_0.921.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the Model"
      ],
      "metadata": {
        "id": "4bGITXDVkp_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights_file = \"mobilenet_v2_v1_03_0.918.pth\"\n",
        "\n",
        "test_ds = ImageFolder(\n",
        "    root=\"./clothing-dataset-small/test\",\n",
        "    transform=val_transforms\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_ds,\n",
        "    batch_size=32,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "model = ClothingClassifier(\n",
        "    size_inner=size, droprate=droprate, num_classes=len(train_ds.classes)\n",
        ")\n",
        "model.load_state_dict(torch.load(weights_file))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "  inputs, labels = inputs.to(device), labels.to(device)\n",
        "  break\n",
        "\n",
        "outputs = model(inputs)\n",
        "total_predictions = labels.size(0)\n",
        "correct_predictions = outputs.argmax(1).eq(labels).sum().item()\n",
        "correct_predictions / total_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF3hIfyaclTc",
        "outputId": "72ad89da-7f59-440d-da25-6ddcd8699680"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8125"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pytorch_preprocessing(X):\n",
        "  X = X / 255.\n",
        "\n",
        "  mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n",
        "  std = np.array([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)\n",
        "\n",
        "  # batch, height, width, channels => batch, channels, height, width\n",
        "  X = X.transpose(0, 3, 1, 2)\n",
        "  X = (X - mean) / std\n",
        "\n",
        "  return X.astype(np.float32)"
      ],
      "metadata": {
        "id": "mmy9LzbJencN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BytesIO\n",
        "from urllib import request\n",
        "from PIL import Image\n",
        "\n",
        "def download_image(url):\n",
        "    with request.urlopen(url) as resp:\n",
        "        buffer = resp.read()\n",
        "    stream = BytesIO(buffer)\n",
        "    img = Image.open(stream)\n",
        "    return img"
      ],
      "metadata": {
        "id": "EzaiTPtvhZb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(img):\n",
        "    if img.mode != 'RGB':\n",
        "        img = img.convert('RGB')\n",
        "    small = img.resize((224, 224), Image.NEAREST) # type: ignore\n",
        "    x = np.array(small, dtype='float32')\n",
        "    batch = np.expand_dims(x, axis=0)\n",
        "    return pytorch_preprocessing(batch)"
      ],
      "metadata": {
        "id": "7AxLogKQhurR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://bit.ly/mlbookcamp-pants'\n",
        "classes = [\n",
        "    \"dress\",\n",
        "    \"hat\",\n",
        "    \"longsleeve\",\n",
        "    \"outwear\",\n",
        "    \"pants\",\n",
        "    \"shirt\",\n",
        "    \"shoes\",\n",
        "    \"shorts\",\n",
        "    \"skirt\",\n",
        "    \"t-shirt\",\n",
        "]\n",
        "\n",
        "img = download_image(url)\n",
        "X = preprocess(img)\n",
        "X = torch.Tensor(X).to(device)\n",
        "\n",
        "pred = np.array(model(X).data[0].cpu())\n",
        "dict(zip(classes, pred.tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhgB9fOEiNVd",
        "outputId": "2d3ac1ae-d89a-42a0-c90d-0a26530d8b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dress': -1.9799416065216064,\n",
              " 'hat': -1.5919891595840454,\n",
              " 'longsleeve': 0.10016340017318726,\n",
              " 'outwear': 0.6754636168479919,\n",
              " 'pants': 6.922043800354004,\n",
              " 'shirt': -1.2410041093826294,\n",
              " 'shoes': 0.10473060607910156,\n",
              " 'shorts': 0.8319987654685974,\n",
              " 'skirt': 0.5130680203437805,\n",
              " 't-shirt': 0.2831695079803467}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exporting the Model to ONNX Format"
      ],
      "metadata": {
        "id": "0ULkc1Q5jgiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxscript"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjFsnZ_HmegS",
        "outputId": "71966674-a66a-488c-837f-cb1814a0fa13"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxscript\n",
            "  Downloading onnxscript-0.5.6-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnxscript) (0.5.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from onnxscript) (2.0.2)\n",
            "Collecting onnx_ir<2,>=0.1.12 (from onnxscript)\n",
            "  Downloading onnx_ir-0.1.12-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting onnx>=1.16 (from onnxscript)\n",
            "  Downloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxscript) (25.0)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (4.15.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.16->onnxscript) (5.29.5)\n",
            "Downloading onnxscript-0.5.6-py3-none-any.whl (683 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m683.0/683.0 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx_ir-0.1.12-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, onnx_ir, onnxscript\n",
            "Successfully installed onnx-1.20.0 onnx_ir-0.1.12 onnxscript-0.5.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.randn(1, 3, input_size, input_size).to(device)\n",
        "onnx_path = \"clothing_classification.onnx\"\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    onnx_path,\n",
        "    verbose=True,\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"output\"],\n",
        "    dynamic_axes={\n",
        "        \"input\": {0: \"batch_size\"},\n",
        "        \"output\": {0: \"batch_size\"}\n",
        "    },\n",
        ")\n",
        "\n",
        "print(f\"Model saved to {onnx_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmshYG5iiimM",
        "outputId": "e9d06df3-0734-4ec7-cb96-099c78d178fd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4023347438.py:4: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
            "  torch.onnx.export(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.onnx] Obtain model graph for `ClothingClassifier([...]` with `torch.export.export(..., strict=False)`...\n",
            "[torch.onnx] Obtain model graph for `ClothingClassifier([...]` with `torch.export.export(..., strict=False)`... ✅\n",
            "[torch.onnx] Run decomposition...\n",
            "[torch.onnx] Run decomposition... ✅\n",
            "[torch.onnx] Translate the graph into ONNX...\n",
            "[torch.onnx] Translate the graph into ONNX... ✅\n",
            "Applied 104 of general pattern rewrite rules.\n",
            "Model saved to clothing_classification.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from onnx import load_model, save_model\n",
        "\n",
        "# load model with external data directory\n",
        "m = load_model(\"clothing_classification.onnx\", load_external_data=True)\n",
        "\n",
        "# force everything into a single file\n",
        "save_model(\n",
        "    m,\n",
        "    \"clothing_classification_single.onnx\",\n",
        "    save_as_external_data=False\n",
        ")"
      ],
      "metadata": {
        "id": "AZrAijIUmMno"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VWyRSJjauK-_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}